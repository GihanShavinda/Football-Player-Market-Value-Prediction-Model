# âš½ Football Player Market Value Predictor
## Machine Learning Assignment

---

## ğŸš€ Quick Start (Do This First!)

### Step 0 â€” Create Python Environment (Recommended)
Use Python 3.11 to avoid Windows build issues with scientific packages.
```bash
py -3.11 -m venv .venv
.venv\Scripts\Activate.ps1
python -m pip install --upgrade pip setuptools wheel
```

### Step 1 â€” Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 2 â€” Download Dataset
1. Go to: https://www.kaggle.com/datasets/stefanoleone992/fifa-22-complete-player-dataset
2. Download `players_22.csv`
3. Place it in the `data/` folder

### Step 3 â€” Run Notebooks IN ORDER
Open each file in Jupyter or VS Code and run all cells:
```bash
# Run these as scripts OR open in Jupyter as notebooks
python notebooks/01_eda.py           # EDA & visualisations
python notebooks/02_preprocessing.py # Clean data & split
python notebooks/03_model_training.py # Train XGBoost (takes ~5 mins)
python notebooks/04_evaluation.py    # Metrics & plots
python notebooks/05_explainability.py # SHAP & XAI
```

### Step 4 â€” Launch the Streamlit App (Bonus 10 marks)
```bash
cd app
streamlit run app.py
```
The app opens automatically at http://localhost:8501

---

## ğŸ“ Project Structure
```
ml_assignment/
â”œâ”€â”€ data/                    # Dataset + generated figures
â”‚   â”œâ”€â”€ players_22.csv       # â† Put your downloaded file here
â”‚   â”œâ”€â”€ X_train.csv          # Generated by notebook 02
â”‚   â”œâ”€â”€ X_val.csv
â”‚   â”œâ”€â”€ X_test.csv
â”‚   â”œâ”€â”€ y_train.csv
â”‚   â”œâ”€â”€ y_val.csv
â”‚   â”œâ”€â”€ y_test.csv
â”‚   â”œâ”€â”€ X_test_raw.csv
â”‚   â””â”€â”€ fig_*.png            # All report figures generated here
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.py            # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_preprocessing.py  # Data cleaning & feature engineering
â”‚   â”œâ”€â”€ 03_model_training.py # XGBoost + Optuna tuning
â”‚   â”œâ”€â”€ 04_evaluation.py     # Metrics, plots, analysis
â”‚   â””â”€â”€ 05_explainability.py # SHAP, Feature Importance, PDP
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ xgb_model.json       # Saved trained model
â”‚   â”œâ”€â”€ scaler.pkl           # Fitted StandardScaler
â”‚   â”œâ”€â”€ feature_names.pkl    # Feature column names
â”‚   â””â”€â”€ best_params.pkl      # Best hyperparameters from Optuna
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py               # Streamlit front-end
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


## ğŸ“Š Figures Generated (Use in Report)
All saved to `data/` after running notebooks:
- `fig_value_distribution.png` â€” raw vs log-transformed target
- `fig_missing_values.png` â€” missing data per column
- `fig_position_analysis.png` â€” players & value by position
- `fig_age_value.png` â€” non-linear age-value relationship
- `fig_correlation.png` â€” feature correlation heatmap
- `fig_learning_curves.png` â€” train vs val RMSE over iterations
- `fig_predictions.png` â€” actual vs predicted + residuals
- `fig_actual_vs_predicted_eur.png` â€” predictions in â‚¬ millions
- `fig_residual_analysis.png` â€” 3-panel residual analysis
- `fig_error_by_position.png` â€” prediction error by position
- `fig_shap_summary.png` â€” SHAP beeswarm (global importance)
- `fig_shap_bar.png` â€” mean |SHAP| bar chart
- `fig_shap_waterfall_high.png` â€” high-value player explanation
- `fig_shap_waterfall_avg.png` â€” average player explanation
- `fig_shap_dependence_age.png` â€” age SHAP dependence
- `fig_shap_dependence_overall.png` â€” overall rating dependence
- `fig_feature_importance_gain.png` â€” XGBoost gain importance
- `fig_pdp.png` â€” Partial Dependence Plots


